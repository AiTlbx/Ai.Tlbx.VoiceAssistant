@page "/"
@using Ai.Tlbx.VoiceAssistant
@using Ai.Tlbx.VoiceAssistant.Provider.OpenAi
@using Ai.Tlbx.VoiceAssistant.Provider.OpenAi.Models
@using Ai.Tlbx.VoiceAssistant.Provider.Google
@using Ai.Tlbx.VoiceAssistant.Provider.Google.Models
@using Ai.Tlbx.VoiceAssistant.WebUi.Components
@using Ai.Tlbx.VoiceAssistant.Demo.Web
@using Ai.Tlbx.VoiceAssistant.Demo.Web.Services
@using Ai.Tlbx.VoiceAssistant.Models
@using Ai.Tlbx.VoiceAssistant.Interfaces
@using System.Diagnostics
@using Microsoft.Extensions.DependencyInjection

@rendermode InteractiveServer
@inject IJSRuntime JS
@inject IServiceProvider ServiceProvider
@inject IVoiceProviderFactory ProviderFactory
@inject IAudioHardwareAccess HardwareAccess
@inject Action<LogLevel, string> LogAction
@implements IAsyncDisposable

<PageTitle>AI Voice Chat</PageTitle>

<CascadingValue Value="voiceAssistant">
    <CascadingValue Value="sessionSettings">
        <div class="min-h-screen bg-gray-50 p-3">
            <div class="max-w-7xl mx-auto">
                <!-- Page Header -->
                <div class="mb-3">
                    <h1 class="text-xl font-semibold text-gray-900">AI Voice Chat</h1>
                </div>

                <div class="flex flex-col lg:flex-row gap-3">
                    <!-- Controls Panel -->
                    <div class="lg:w-1/3">
                        <div class="bg-white rounded-lg shadow-sm border border-gray-200">
                            <div class="border-b border-gray-200 px-3 py-2">
                                <h3 class="text-sm font-medium text-gray-700">Controls</h3>
                            </div>

                            <div class="p-3 space-y-3">
                                <!-- Talk Button -->
                                <AiTalkControl OnStartTalking="StartSession" OnStopTalking="StopSession" IsTalking="voiceAssistant?.IsRecording ?? false" Loading="(voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false)" />

                                <!-- Provider Selection -->
                                <div>
                                    <label for="providerSelect" class="block text-xs font-medium text-gray-600 mb-1">AI Provider</label>
                                    <ProviderSelect AvailableProviders="@availableProviders" @bind-SelectedProvider="@selectedProviderString" Disabled="@((voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false))" />
                                </div>

                                <!-- Voice Selection -->
                                <div>
                                    <label for="voiceSelect" class="block text-xs font-medium text-gray-600 mb-1">Voice</label>
                                    <VoiceSelect AvailableVoices="@availableVoices" @bind-SelectedVoice="@currentVoiceString" Disabled="@((voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false))" />
                                </div>

                                <!-- Voice Speed -->
                                <VoiceSpeedSlider SelectedSpeed="@currentVoiceSpeed" SelectedSpeedChanged="OnVoiceSpeedChanged" Disabled="@((voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false))" />

                                <!-- Tool Selection -->
                                <div>
                                    <ToolSelector AvailableTools="@availableTools"
                                                 @bind-EnabledTools="@enabledTools"
                                                 Disabled="@((voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false))" />
                                </div>

                                <!-- Microphone Selection -->
                                <div>
                                    <label class="block text-xs font-medium text-gray-600 mb-1">Microphone</label>
                                    <MicrophoneSelect AvailableMicrophones="availableMicrophones"
                                                     @bind-SelectedMicrophoneId="selectedMicrophoneId"
                                                     MicPermissionGranted="micPermissionGranted"
                                                     OnRequestPermission="RequestMicrophonePermission"
                                                     Disabled="@((voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false))" />
                                </div>

                                <!-- Action Buttons -->
                                <div class="space-y-2">
                                    <button class="w-full py-2 px-3 rounded-md bg-gray-600 text-white text-sm font-medium hover:bg-gray-700 transition-colors flex items-center justify-center"
                                    @onclick="ClearChat"
                                    disabled="@((voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false))">
                                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1.5" viewBox="0 0 20 20" fill="currentColor">
                                            <path fill-rule="evenodd" d="M9 2a1 1 0 00-.894.553L7.382 4H4a1 1 0 000 2v10a2 2 0 002 2h8a2 2 0 002-2V6a1 1 0 100-2h-3.382l-.724-1.447A1 1 0 0011 2H9zM7 8a1 1 0 012 0v6a1 1 0 11-2 0V8zm5-1a1 1 0 00-1 1v6a1 1 0 102 0V8a1 1 0 00-1-1z" clip-rule="evenodd" />
                                        </svg>
                                        Clear Chat
                                    </button>

                                    <MicTestWidget OnStartTest="TestMicrophone" Loading="(voiceAssistant?.IsMicrophoneTesting ?? false) || (voiceAssistant?.IsConnecting ?? false) || (voiceAssistant?.IsRecording ?? false)" />
                                </div>

                                <!-- Status -->
                                <div>
                                    <StatusWidget ConnectionStatus="@(voiceAssistant?.ConnectionStatus ?? "")" Error="@(voiceAssistant?.LastErrorMessage ?? "")" IsMicrophoneTesting="@(voiceAssistant?.IsMicrophoneTesting ?? false)" />
                                </div>

                                <!-- Connection Events -->
                                @if (!string.IsNullOrEmpty(lastConnectionEvent))
                                {
                                    <div class="border-t pt-3">
                                        <label class="block text-xs font-medium text-gray-600 mb-1">Last Event</label>
                                        <div class="text-xs text-gray-700 bg-gray-50 rounded px-2 py-1">
                                            @lastConnectionEvent
                                        </div>
                                    </div>
                                }
                            </div>
                        </div>
                    </div>

                    <!-- Chat Panel -->
                    <div class="lg:w-2/3">
                        <div class="bg-white rounded-lg shadow-sm border border-gray-200 h-[500px] flex flex-col overflow-hidden">
                            <div class="border-b border-gray-200 px-3 py-2 flex items-center justify-between">
                                <h3 class="text-sm font-medium text-gray-700">Conversation</h3>
                                <DebugModeToggle @bind-ShowDebugMode="showDebugMode" />
                            </div>

                            <div class="flex-1 p-3 overflow-y-auto bg-gray-50" id="chat-messages" @ref="chatContainer">
                                <ChatWidget ShowDebugToolCalls="@showDebugMode" ShowToolCalls="true" />
                            </div>

                            <div class="px-3 py-2 border-t border-gray-200 bg-white">
                                @if (voiceAssistant?.IsRecording ?? false)
                                {
                                    <div class="flex items-center text-red-600 text-sm">
                                        <div class="relative h-2 w-2 mr-2">
                                            <span class="animate-ping absolute h-2 w-2 rounded-full bg-red-400 opacity-75"></span>
                                            <span class="absolute h-2 w-2 rounded-full bg-red-500"></span>
                                        </div>
                                        <span>Listening...</span>
                                    </div>
                                }
                                else
                                {
                                    <span class="text-gray-500 text-sm">Ready</span>
                                }
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </CascadingValue>
</CascadingValue>

@code
{
    private VoiceAssistant? voiceAssistant;
    private ElementReference chatContainer;
    private bool showDebugMode = false;

    // Provider management
    private List<string> availableProviders = new() { "OpenAI", "Google" };
    private string _selectedProviderString = "OpenAI";
    private string selectedProviderString
    {
        get => _selectedProviderString;
        set
        {
            if (_selectedProviderString != value && voiceAssistant != null && !voiceAssistant.IsConnecting && !voiceAssistant.IsRecording)
            {
                _selectedProviderString = value;
                Log(LogLevel.Info, $"Provider changed to: {_selectedProviderString}");
                UpdateAvailableVoicesForProvider();
                RecreateVoiceAssistant();
            }
        }
    }

    // Voice management (provider-specific)
    private string _currentVoiceString = "Alloy";
    private string currentVoiceString
    {
        get => _currentVoiceString;
        set
        {
            _currentVoiceString = value;
            sessionSettings = sessionSettings with { SelectedVoice = value };
            Log(LogLevel.Info, $"Voice changed to: {_currentVoiceString}");
        }
    }
    private double currentVoiceSpeed = 1.0;
    private List<string> availableVoices = Enum.GetValues<AssistantVoice>().Select(v => v.ToString()).ToList();

    // Tool management
    private List<IVoiceTool> availableTools = new();
    private List<IVoiceTool> enabledTools = new();

    // Microphone selection related fields
    private List<MicrophoneSelect.MicrophoneInfo> availableMicrophones = new();
    private string selectedMicrophoneId = string.Empty;
    private bool micPermissionGranted = false;

    private SessionSettingsContext sessionSettings = new();
    private string lastConnectionEvent = string.Empty;

    protected override async Task OnInitializedAsync()
    {
        // Create initial VoiceAssistant with OpenAI provider
        RecreateVoiceAssistant();

        // Get all registered tools from DI
        availableTools = ServiceProvider.GetServices<IVoiceTool>().ToList();
        Log(LogLevel.Info, $"Found {availableTools.Count} available tools: {string.Join(", ", availableTools.Select(t => t.Name))}");

        // Enable all tools by default
        enabledTools = availableTools.ToList();

        await Task.CompletedTask;
    }

    private void RecreateVoiceAssistant()
    {
        // Dispose old instance if exists
        if (voiceAssistant != null)
        {
            voiceAssistant.OnConnectionStatusChanged = null;
            voiceAssistant.OnMessageAdded = null;
            voiceAssistant.OnMicrophoneDevicesChanged = null;
        }

        // Create provider
        var providerType = _selectedProviderString == "OpenAI" ? VoiceProviderType.OpenAI : VoiceProviderType.Google;
        var provider = ProviderFactory.CreateProvider(providerType);

        // Create new VoiceAssistant
        voiceAssistant = new VoiceAssistant(HardwareAccess, provider, LogAction);

        // Wire up callbacks
        voiceAssistant.OnConnectionStatusChanged = OnConnectionStatusChanged;
        voiceAssistant.OnMessageAdded = OnMessageAdded;
        voiceAssistant.OnMicrophoneDevicesChanged = OnMicrophoneDevicesChanged;

        Log(LogLevel.Info, $"Voice Assistant recreated with {_selectedProviderString} provider");
    }

    // Logging method using the centralized logging system
    private void Log(LogLevel level, string message)
    {
        // Direct logging is not exposed in VoiceAssistant
        Debug.WriteLine($"WebDemoLog: [{level}] [Home] {message}");
    }

    private void UpdateAvailableVoicesForProvider()
    {
        if (_selectedProviderString == "OpenAI")
        {
            availableVoices = Enum.GetValues<AssistantVoice>().Select(v => v.ToString()).ToList();
            _currentVoiceString = "Alloy";
        }
        else if (_selectedProviderString == "Google")
        {
            availableVoices = Enum.GetValues<GoogleVoice>().Select(v => v.ToString()).ToList();
            _currentVoiceString = "Aoede";
        }
    }

    private async Task CheckMicrophonePermission()
    {
        try
        {
            if (voiceAssistant == null) return;

            // Try to get microphones - this will succeed if permission is already granted
            var mics = await voiceAssistant.GetAvailableMicrophonesAsync();

            // Check if we have actual device information - actual device labels mean permission is granted
            bool hasRealDeviceNames = mics.Count > 0 && 
                mics.Any(m => !string.IsNullOrEmpty(m.Name) && !m.Name.StartsWith("Microphone "));

            micPermissionGranted = hasRealDeviceNames;
            Log(LogLevel.Info, $"Initial permission check: Permission granted: {micPermissionGranted}, Found {mics.Count} microphones");

            if (micPermissionGranted)
            {
                // If we got devices with real names, permission is granted
                availableMicrophones = mics.Select(m => new MicrophoneSelect.MicrophoneInfo
                {
                    Id = m.Id,
                    Name = m.Name,
                    IsDefault = m.IsDefault
                }).ToList();

                // Select default or first microphone
                if (availableMicrophones.Count > 0)
                {
                    var defaultMic = availableMicrophones.FirstOrDefault(m => m.IsDefault);
                    selectedMicrophoneId = defaultMic?.Id ?? availableMicrophones[0].Id;
                    // Note: SetMicrophoneDevice might need to be implemented differently

                    Log(LogLevel.Info, $"Initially selected microphone: {availableMicrophones.FirstOrDefault(m => m.Id == selectedMicrophoneId)?.Name ?? "Unknown"}");
                }
            }
            else if (mics.Count > 0)
            {
                // We have devices but without proper labels - this suggests permission is needed
                // but we still store them for when permission is granted
                availableMicrophones = mics.Select(m => new MicrophoneSelect.MicrophoneInfo
                {
                    Id = m.Id,
                    Name = m.Name,
                    IsDefault = m.IsDefault
                }).ToList();
                Log(LogLevel.Info, "Microphones found but without proper labels - permission needed");
            }
            else
            {
                Log(LogLevel.Warn, "No microphones found or permission not granted");
            }
        }
        catch (Exception ex)
        {
            Log(LogLevel.Error, $"Error checking microphone permission: {ex.Message}");
            Log(LogLevel.Error, $"Stack trace: {ex.StackTrace}");
            micPermissionGranted = false;
        }
        finally
        {
            await InvokeAsync(StateHasChanged);
        }
    }

    private async Task CheckMicrophonePermissionState()
    {
        try
        {
            if (voiceAssistant == null) return;

            // Just get available microphones - the JS will check permissions without activating
            var mics = await voiceAssistant.GetAvailableMicrophonesAsync();

            // Check if we have actual device information - actual device labels mean permission is granted
            bool hasRealDeviceNames = mics.Count > 0 && 
                mics.Any(m => !string.IsNullOrEmpty(m.Name) && !m.Name.StartsWith("Microphone "));

            micPermissionGranted = hasRealDeviceNames;
            Log(LogLevel.Info, $"Permission check: Permission granted: {micPermissionGranted}, Found {mics.Count} microphones");

            if (mics.Count > 0)
            {
                availableMicrophones = mics.Select(m => new MicrophoneSelect.MicrophoneInfo
                {
                    Id = m.Id,
                    Name = m.Name,
                    IsDefault = m.IsDefault
                }).ToList();

                // Select default or first microphone
                var defaultMic = availableMicrophones.FirstOrDefault(m => m.IsDefault);
                if (defaultMic != null)
                {
                    selectedMicrophoneId = defaultMic.Id;
                    Log(LogLevel.Info, $"Selected default microphone: {defaultMic.Name}");
                }
                else if (availableMicrophones.Count > 0)
                {
                    selectedMicrophoneId = availableMicrophones[0].Id;
                    Log(LogLevel.Info, $"Selected first microphone: {availableMicrophones[0].Name}");
                }
            }
            else
            {
                Log(LogLevel.Info, "No microphones found or permission not granted");
            }
        }
        catch (Exception ex)
        {
            Log(LogLevel.Error, $"Error checking microphone permission state: {ex.Message}");
            micPermissionGranted = false;
        }
        finally
        {
            await InvokeAsync(StateHasChanged);
        }
    }

    private async Task RequestMicrophonePermission()
    {
        try
        {
            if (voiceAssistant == null) return;

            // Instead of setting isLoadingMicrophones, we'll rely on micPermissionGranted for UI state
            await InvokeAsync(StateHasChanged);

            Log(LogLevel.Info, "Explicitly requesting microphone permission...");

            // Use the new method that explicitly requests permission and gets labeled devices
            // Request permission by getting devices
            var deviceInfos = await voiceAssistant.GetAvailableMicrophonesAsync();

            // Convert to MicrophoneInfo objects
            availableMicrophones = deviceInfos.Select(m => new MicrophoneSelect.MicrophoneInfo
            {
                Id = m.Id,
                Name = m.Name,
                IsDefault = m.IsDefault
            }).ToList();

            // Check if we now have devices with real names (not placeholders)
            micPermissionGranted = deviceInfos.Count > 0 && 
                deviceInfos.Any(m => !string.IsNullOrEmpty(m.Name) && !m.Name.StartsWith("Microphone "));

            Log(LogLevel.Info, $"After permission request: Permission granted: {micPermissionGranted}, Found {deviceInfos.Count} microphones");

            // If we have microphones with permission, select one
            if (micPermissionGranted && availableMicrophones.Count > 0)
            {
                // Try to find default microphone
                var defaultMic = availableMicrophones.FirstOrDefault(m => m.IsDefault);
                if (defaultMic != null)
                {
                    selectedMicrophoneId = defaultMic.Id;
                }
                else if (availableMicrophones.Count > 0)
                {
                    // Otherwise use the first microphone
                    selectedMicrophoneId = availableMicrophones[0].Id;
                }

                // Set the selected device
                if (!string.IsNullOrEmpty(selectedMicrophoneId))
                {
                    // Note: SetMicrophoneDevice might need to be implemented differently
                    var selectedMic = availableMicrophones.FirstOrDefault(m => m.Id == selectedMicrophoneId);
                    Log(LogLevel.Info, $"Selected microphone: {selectedMic?.Name ?? "Unknown"}");
                }
            }
            else
            {
                Log(LogLevel.Warn, "Permission not granted or no microphones with labels found");
            }
        }
        catch (Exception ex)
        {
            Log(LogLevel.Error, $"Error requesting microphone permission: {ex.Message}");
            Log(LogLevel.Error, $"Stack trace: {ex.StackTrace}");
        }
        finally
        {
            // isLoadingMicrophones = false;
            await InvokeAsync(StateHasChanged);
        }
    }


    private async Task OnVoiceSpeedChanged(double newSpeed)
    {
        currentVoiceSpeed = newSpeed;
        sessionSettings = sessionSettings with { SelectedSpeed = newSpeed };
        await Task.CompletedTask;
    }

    private void OnMicrophoneDevicesChanged(List<AudioDeviceInfo> devices)
    {
        InvokeAsync(async () => 
        {
            availableMicrophones = devices.Select(m => new MicrophoneSelect.MicrophoneInfo
            {
                Id = m.Id,
                Name = m.Name,
                IsDefault = m.IsDefault
            }).ToList();
            await Task.CompletedTask;
            StateHasChanged();
        });
    }

    private void OnConnectionStatusChanged(string status)
    {
        InvokeAsync(() => 
        {
            if (!string.IsNullOrEmpty(status))
            {
                lastConnectionEvent = $"{DateTime.Now:HH:mm:ss} - {status}";
            }
            StateHasChanged();
        });
    }

    private void OnMessageAdded(ChatMessage message)
    {
        InvokeAsync(async () => {
            StateHasChanged();
            await Task.Delay(50); // Give the DOM time to update
            await ScrollToBottom();
        });
    }

    private async Task ScrollToBottom()
    {
        try
        {
            await JS.InvokeVoidAsync("scrollToBottom", chatContainer);
        }
        catch
        {
            // Ignore JS interop errors
        }
    }

    private async Task StartSession()
    {
        try
        {
            if (voiceAssistant == null)
            {
                Log(LogLevel.Error, "VoiceAssistant not initialized");
                return;
            }

            // Store the currently selected device before starting the session
            string deviceToUse = selectedMicrophoneId;
            string deviceName = availableMicrophones.FirstOrDefault(m => m.Id == deviceToUse)?.Name ?? "default device";

            // Apply the selected microphone if it has a value
            if (!string.IsNullOrEmpty(deviceToUse))
            {
                Log(LogLevel.Info, $"Using microphone: {deviceName} for recording session");
            }

            // Create provider-specific settings
            IVoiceSettings settings;

            var instructions = "Du bist ein freundlicher hilfreicher sprachagent. (Antworte in der Anfragesprache, im Zweifel bitte in Deutsch), Nimm an dein Gegenüber ist ein Experte in der Materie über die ihr gerade redet, also neige eher zu sehr fachlichen Antworten und mach die antworten nur auf Nachfrage leichter verständlich. Sprich schnell mit möglichst kurzen pausen, bring emotion in deine Stimme, lache wenn es passt.";

            if (_selectedProviderString == "OpenAI")
            {
                if (!Enum.TryParse<AssistantVoice>(_currentVoiceString, out var openAiVoice))
                {
                    openAiVoice = AssistantVoice.Alloy;
                }

                settings = new OpenAiVoiceSettings
                {
                    Instructions = instructions,
                    Voice = openAiVoice,
                    InputAudioTranscription = new InputAudioTranscription
                    {
                        Model = "gpt-4o-transcribe",
                        Prompt = "Expect German with slight accent",
                        Enabled = true,
                    },
                    TalkingSpeed = currentVoiceSpeed,
                    Model = OpenAiRealtimeModel.Gpt520250828,
                    Tools = enabledTools.ToList()
                };

                Log(LogLevel.Info, $"Starting OpenAI session with voice: {openAiVoice}");
            }
            else // Google
            {
                if (!Enum.TryParse<GoogleVoice>(_currentVoiceString, out var googleVoice))
                {
                    googleVoice = GoogleVoice.Aoede;
                }

                settings = new GoogleVoiceSettings
                {
                    Instructions = instructions,
                    Voice = googleVoice,
                    TalkingSpeed = currentVoiceSpeed,
                    Model = GoogleModel.Gemini20FlashLive001,
                    ResponseModality = "AUDIO",
                    LanguageCode = "de-DE", // Set to German - change to "en-US" for English or null for auto-detect
                    Tools = enabledTools.ToList(),
                    TranscriptionConfig = new AudioTranscriptionConfig
                    {
                        EnableInputTranscription = true,
                        EnableOutputTranscription = true
                    }
                };

                Log(LogLevel.Info, $"Starting Google session with voice: {googleVoice}");
            }

            // Start the session with the configured settings
            await voiceAssistant.StartAsync(settings);

            // Even if permission state is lost in the process, we still know we have a valid device
            if (!micPermissionGranted && availableMicrophones.Count > 0)
            {
                micPermissionGranted = true;
            }

            await InvokeAsync(StateHasChanged);
        }
        catch (Exception ex)
        {
            Log(LogLevel.Error, $"Error starting session: {ex.Message}");
            Log(LogLevel.Error, $"Stack trace: {ex.StackTrace}");
        }
    }

    private async Task StopSession()
    {
        try
        {
            if (voiceAssistant == null) return;

            await voiceAssistant.StopAsync();

            // Ensure mic permission state is preserved after stopping
            if (!micPermissionGranted && availableMicrophones.Count > 0)
            {
                micPermissionGranted = true;
            }

            await InvokeAsync(StateHasChanged);
        }
        catch (Exception ex)
        {
            Log(LogLevel.Error, $"Error stopping session: {ex.Message}");
            Log(LogLevel.Error, $"Stack trace: {ex.StackTrace}");
        }
    }    

    private void ClearChat()
    {
        voiceAssistant?.ClearChatHistory();
        InvokeAsync(StateHasChanged);
    }

    private async Task TestMicrophone()
    {
        try
        {
            if (voiceAssistant == null)
            {
                Log(LogLevel.Error, "VoiceAssistant not initialized");
                return;
            }

            // Store the currently selected device before testing
            string deviceToUse = selectedMicrophoneId;
            string deviceName = availableMicrophones.FirstOrDefault(m => m.Id == deviceToUse)?.Name ?? "default device";

            // Apply the selected microphone if it has a value
            if (!string.IsNullOrEmpty(deviceToUse))
            {
                Log(LogLevel.Info, $"Testing microphone: {deviceName}");
            }

            await voiceAssistant.TestMicrophoneAsync();

            // Even if permission state is lost in the process, we still know we have a valid device
            if (!micPermissionGranted && availableMicrophones.Count > 0)
            {
                micPermissionGranted = true;
            }

            await InvokeAsync(StateHasChanged);
        }
        catch (Exception ex)
        {
            Log(LogLevel.Error, $"Error testing microphone: {ex.Message}");
            Log(LogLevel.Error, $"Stack trace: {ex.StackTrace}");
        }
    }

    public async ValueTask DisposeAsync()
    {
        if (voiceAssistant != null)
        {
            voiceAssistant.OnConnectionStatusChanged = null;
            voiceAssistant.OnMessageAdded = null;
            voiceAssistant.OnMicrophoneDevicesChanged = null;
            await voiceAssistant.DisposeAsync();
        }
    }

    protected override async Task OnAfterRenderAsync(bool firstRender)
    {
        if (firstRender)
        {
            // Add JS helper function for scrolling
            await JS.InvokeVoidAsync("eval", @"
                window.scrollToBottom = function(element) {
                    if (element) {
                        element.scrollTop = element.scrollHeight;
                    }
                }
            ");

            // Check microphone permission state without activating microphone
            await CheckMicrophonePermissionState();
        }
    }

    // Helper function to format JSON for display
    private string FormatJson(string? jsonString)
    {
        if (string.IsNullOrWhiteSpace(jsonString))
        {    
            return "(empty)";
        }
        try
        {
            using var jsonDoc = System.Text.Json.JsonDocument.Parse(jsonString);
            return System.Text.Json.JsonSerializer.Serialize(jsonDoc, new System.Text.Json.JsonSerializerOptions { WriteIndented = true });
        }
        catch (System.Text.Json.JsonException)
        {
            // If it's not valid JSON, return the original string
            return jsonString;
        }
        catch (Exception ex)
        {
            return $"Error formatting JSON: {ex.Message}";
        }
    }

    // Helper function to truncate text
    private string TruncateText(string text, int maxLength)
    {
        if (string.IsNullOrEmpty(text) || text.Length <= maxLength)
            return text;
            
        return text.Substring(0, maxLength) + "...";
    }
}

